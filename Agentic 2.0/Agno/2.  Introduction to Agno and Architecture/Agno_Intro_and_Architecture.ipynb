{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcf4f1b",
   "metadata": {},
   "source": [
    "# Introduction to Agno & Architecture (Student Copy)\n",
    "\n",
    "**Goal:** Understand Agno’s core concepts and system architecture so you can reason about how agents, teams, workflows, tools, memory, and the runtime (AgentOS) fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9028f74",
   "metadata": {},
   "source": [
    "## 1) High‑Level Overview\n",
    "\n",
    "Agno = **multi‑agent framework** + **runtime (AgentOS)** + **control plane (web UI)**.\n",
    "\n",
    "- Build **Agents**, combine them into **Teams** (autonomy), or orchestrate with **Workflows** (control).  \n",
    "- Run everything on **AgentOS (FastAPI)**.  \n",
    "- Use the **AgentOS UI** to test, monitor, and manage in real time.  \n",
    "- **Private by design**: The runtime lives in your environment; the UI connects directly (no external data retention).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a09b4",
   "metadata": {},
   "source": [
    "### Big‑Picture Architecture\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Client\n",
    "        U[User / App / CLI]\n",
    "        UI[AgentOS UI Browser]\n",
    "    end\n",
    "\n",
    "    subgraph Runtime\n",
    "        OS[AgentOS FastAPI]\n",
    "        ST[(Persistent Storage<br/>DB / Files / Vector DB)]\n",
    "    end\n",
    "\n",
    "    subgraph AgentsLayer[Agents / Teams / Workflows]\n",
    "        A1[Agent]\n",
    "        T1[Team]\n",
    "        W1[Workflow]\n",
    "    end\n",
    "\n",
    "    subgraph Ext[External Integrations]\n",
    "        TOOLS[Tools / APIs / Services]\n",
    "        MCP[(MCP Servers)]\n",
    "        KNOW[(Knowledge / Vector Stores)]\n",
    "    end\n",
    "\n",
    "    U <-->|HTTP / SSE| OS\n",
    "    UI <-->|WS / SSE| OS\n",
    "    OS --> A1\n",
    "    OS --> T1\n",
    "    OS --> W1\n",
    "    A1 <-->|state/history| ST\n",
    "    T1 <-->|shared ctx| ST\n",
    "    W1 <-->|session state| ST\n",
    "    A1 -->|tool calls| TOOLS\n",
    "    A1 -->|MCP| MCP\n",
    "    A1 -->|RAG| KNOW\n",
    "    \n",
    "    style U fill:#e1f5fe\n",
    "    style UI fill:#e1f5fe\n",
    "    style OS fill:#f3e5f5\n",
    "    style ST fill:#e3f2fd\n",
    "    style A1 fill:#e8f5e8\n",
    "    style T1 fill:#e8f5e8\n",
    "    style W1 fill:#e8f5e8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9474163",
   "metadata": {},
   "source": [
    "## 2) Core Building Blocks\n",
    "\n",
    "| Component | What it is | Why it matters |\n",
    "|---|---|---|\n",
    "| **Agent** | An LLM‑powered actor with instructions, tools, memory. | Encapsulates role, reasoning, and actions for a task. |\n",
    "| **Team** | A group of agents collaborating with shared context. | Increases autonomy and specialization (researcher + analyst, etc.). |\n",
    "| **Workflow** | A deterministic, step‑based orchestration. | Adds explicit control: branching, looping, parallel steps. |\n",
    "| **AgentOS** | FastAPI runtime that serves agents/teams/workflows. | Production‑ready API, streaming endpoints, scaling. |\n",
    "| **Control Plane (UI)** | Web UI connecting to your AgentOS. | Test, monitor, and debug runs, tools, and memory live. |\n",
    "| **Tools** | Built‑in or custom functions/APIs the agent can call. | Extend capability beyond LLM knowledge (web, data, code). |\n",
    "| **Memory** | User/session state persisted to storage. | Context across turns; personalization. |\n",
    "| **Knowledge** | Vector‑store backed retrieval (RAG). | Ground answers in your docs/data. |\n",
    "| **MCP** | Model Context Protocol integration. | Standard way to connect external systems securely. |\n",
    "| **Hooks & Guardrails** | Pre/post validation & safety mechanisms. | Enforce structure, protect prompts, confirm actions. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84cfee",
   "metadata": {},
   "source": [
    "## 3) Orchestration Styles: Team vs Workflow\n",
    "\n",
    "Use **Teams** when you want **agent autonomy and collaboration**.  \n",
    "Use **Workflows** when you need **explicit sequences and routing**.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Team\n",
    "        UQ[User Query] --> LEAD[Team Leader]\n",
    "        LEAD --> R1[Agent A: Research]\n",
    "        LEAD --> R2[Agent B: Analyze]\n",
    "        LEAD --> R3[Agent C: Write]\n",
    "        R1 -->|share| LEAD\n",
    "        R2 -->|share| LEAD\n",
    "        R3 -->|share| LEAD\n",
    "        LEAD --> OUT[Final Answer]\n",
    "    end\n",
    "    \n",
    "    style UQ fill:#e1f5fe\n",
    "    style LEAD fill:#fff3e0\n",
    "    style R1 fill:#e8f5e8\n",
    "    style R2 fill:#e8f5e8\n",
    "    style R3 fill:#e8f5e8\n",
    "    style OUT fill:#e3f2fd\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph Workflow\n",
    "    A[Step 1: Parse Intent] -->|branch| B{Router}\n",
    "    B -->|code| C[Step 2: Coding Agent]\n",
    "    B -->|general| D[Step 2: QA Agent]\n",
    "    C --> E[Step 3: Tests]\n",
    "    D --> E\n",
    "    E --> F[Step 4: Format / Validate]\n",
    "  end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9ad0a",
   "metadata": {},
   "source": [
    "### Quick Comparison\n",
    "\n",
    "| Aspect | Team | Workflow |\n",
    "|---|---|---|\n",
    "| **Control** | Emergent, agent‑driven | Explicit, developer‑defined |\n",
    "| **When to use** | Open‑ended tasks, collaboration | Deterministic pipelines, forms, routing |\n",
    "| **Parallelism** | Natural via multiple agents | Explicit (mark steps parallel) |\n",
    "| **Debuggability** | Use UI traces to see dialogue | Inspect step outputs and branches |\n",
    "| **Maintenance** | Adjust roles/instructions | Adjust DAG (steps/conditions) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c94773",
   "metadata": {},
   "source": [
    "## 4) Inside an Agent\n",
    "\n",
    "**Key parts**  \n",
    "- **Model**: any supported LLM provider (model‑agnostic).  \n",
    "- **Instructions**: role/system prompt that sets behavior.  \n",
    "- **Tools**: functions/APIs the agent may invoke.  \n",
    "- **Structured I/O**: `input_schema` / `output_schema` for predictable JSON.  \n",
    "- **Memory**: user/session history and state persisted to storage.  \n",
    "- **Markdown / Multimodal**: format output, handle images/audio/files.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph Agent\n",
    "    I[Instructions] --> LLM[(Model)]\n",
    "    SCH[Input / Output Schemas] --> LLM\n",
    "    MEM[(Memory/State)] --> LLM\n",
    "    LLM --> O[Answer]\n",
    "    LLM --> TC{Tool Call?}\n",
    "    TC --> TOOL[Tool]\n",
    "    TOOL --> LLM\n",
    "  end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07888",
   "metadata": {},
   "source": [
    "### Structured I/O (type‑safe)\n",
    "\n",
    "| Feature | Purpose | Example |\n",
    "|---|---|---|\n",
    "| **`input_schema`** | Validate/shape incoming params | Enforce required fields/types |\n",
    "| **`output_schema`** | Force JSON output structure | `{title:str, rating:float, url:str}` |\n",
    "| **Validation hooks** | Post‑process results | Reject/massage invalid values |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a92c7a",
   "metadata": {},
   "source": [
    "### Reference: Simple Agent with Structured Output (do not run without creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat  # provider example\n",
    "\n",
    "class MovieSuggestion(BaseModel):\n",
    "    title: str = Field(..., description=\"Movie title\")\n",
    "    genre: str\n",
    "    rating: float\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Recommender\",\n",
    "    model=OpenAIChat(id=\"gpt-3.5-turbo\"),\n",
    "    output_schema=MovieSuggestion,\n",
    "    instructions=\"Recommend one movie that fits the user's mood.\"\n",
    ")\n",
    "\n",
    "print(agent.run(\"I want a light-hearted comedy tonight.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73cfca",
   "metadata": {},
   "source": [
    "## 5) State, Memory & Persistence\n",
    "\n",
    "- **Session:** connects turns in a conversation (via `session_id`).  \n",
    "- **User memory:** store user‑specific facts across sessions (`user_id`).  \n",
    "- **Persistent storage:** SQLite / files / other DBs for history & state.  \n",
    "- **Knowledge (RAG):** connect 20+ vector stores to retrieve relevant docs.  \n",
    "- **Culture:** shared, compounding knowledge across agents.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph Persistence\n",
    "    DB[(Relational/JSON)]\n",
    "    VS[(Vector Store)]\n",
    "  end\n",
    "  AG[Agent] -->|save history| DB\n",
    "  AG -->|retrieve chunks| VS\n",
    "  T[Team] -->|shared ctx| DB\n",
    "  W[Workflow] -->|session state| DB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4deb7c4",
   "metadata": {},
   "source": [
    "## 6) AgentOS Runtime & Scaling\n",
    "\n",
    "**AgentOS** is a **FastAPI** runtime with **async** endpoints and **SSE** streaming.\n",
    "\n",
    "- **Stateless at runtime** → horizontally scalable behind a load balancer.  \n",
    "- **Low overhead** → microseconds agent instantiation, tiny memory footprint.  \n",
    "- **Private by design** → run entirely in your cloud; UI connects directly.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph Cluster\n",
    "    LB[Load Balancer]\n",
    "    S1[AgentOS Instance 1]\n",
    "    S2[AgentOS Instance 2]\n",
    "    S3[AgentOS Instance 3]\n",
    "  end\n",
    "  LB --> S1\n",
    "  LB --> S2\n",
    "  LB --> S3\n",
    "  S1 --- ST[(Shared Storage/DB)]\n",
    "  S2 --- ST\n",
    "  S3 --- ST\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50816f9",
   "metadata": {},
   "source": [
    "## 7) Tools & Integrations\n",
    "\n",
    "- **Built‑in toolkits**: 100+ toolkits across web/data/code/enterprise.  \n",
    "- **Custom tools**: any Python function (or `@tool` decorator).  \n",
    "- **MCP**: first‑class support to connect external systems safely.  \n",
    "- **Best practices**: clear docstrings; require confirmation for sensitive actions; add caching for expensive calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference only: add a simple custom tool and a built-in tool\n",
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    # Replace with real API in production\n",
    "    return f\"{symbol} is trading at $123.45 (demo)\"\n",
    "\n",
    "finance_agent = Agent(\n",
    "    name=\"FinanceHelper\",\n",
    "    model=OpenAIChat(id=\"gpt-3.5-turbo\"),\n",
    "    tools=[DuckDuckGoTools(), get_stock_price],\n",
    "    instructions=\"Use web search and the stock tool when helpful; answer clearly.\"\n",
    ")\n",
    "\n",
    "print(finance_agent.run(\"What is the latest on NVIDIA and how is NVDA trading?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb76db",
   "metadata": {},
   "source": [
    "## 8) Security, Governance & Reliability\n",
    "\n",
    "- **Guardrails:** validate outputs, protect prompts, enforce policies.  \n",
    "- **Human‑in‑the‑Loop:** confirmations/overrides for critical actions.  \n",
    "- **Access control:** RBAC and per‑agent permissions.  \n",
    "- **Evals:** measure **accuracy**, **performance**, **reliability** continuously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b260bb",
   "metadata": {},
   "source": [
    "## 9) Architecture Quick‑Reference\n",
    "\n",
    "### Layers\n",
    "| Layer | Responsibilities |\n",
    "|---|---|\n",
    "| **Client/UI** | Input, visualization, streaming output, traces |\n",
    "| **AgentOS** | API, auth, routing, streaming, scaling |\n",
    "| **Agents/Teams/Workflows** | Reasoning, collaboration, orchestration |\n",
    "| **Tools/Knowledge/MCP** | External actions and retrieval |\n",
    "| **Storage** | Sessions, history, state, vector indexes |\n",
    "\n",
    "### Performance Focus\n",
    "| Dimension | What Agno optimizes |\n",
    "|---|---|\n",
    "| **Agent** | Instantiation time, low memory, efficient history |\n",
    "| **System** | Async API, stateless scale‑out, background tasks |\n",
    "| **Reliability** | Evals, schema validation, hooks/guardrails |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605bd43",
   "metadata": {},
   "source": [
    "## 10) Minimal Runtime References (do not run without setup)\n",
    "\n",
    "**Single agent + AgentOS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ea617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.anthropic import Claude\n",
    "from agno.os import AgentOS\n",
    "\n",
    "agent = Agent(name=\"Helper\", model=Claude(id=\"claude-sonnet-4-5\"), markdown=True)\n",
    "agent_os = AgentOS(agents=[agent])\n",
    "app = agent_os.get_app()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent_os.serve(app=\"app:app\", reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d3ea7",
   "metadata": {},
   "source": [
    "**Team example (sketch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ec23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "from agno.team import Team\n",
    "\n",
    "researcher = Agent(name=\"Researcher\", model=OpenAIChat(id=\"gpt-3.5-turbo\"))\n",
    "analyst    = Agent(name=\"Analyst\",    model=OpenAIChat(id=\"gpt-3.5-turbo\"))\n",
    "writer     = Agent(name=\"Writer\",     model=OpenAIChat(id=\"gpt-3.5-turbo\"))\n",
    "\n",
    "qa_team = Team(members=[researcher, analyst, writer],\n",
    "               instructions=\"Work together: research, analyze, then write a concise answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e95e07",
   "metadata": {},
   "source": [
    "**Workflow example (sketch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea81c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode-style sketch for a workflow class (API varies by version)\n",
    "# class MyWorkflow(Workflow):\n",
    "#     step1 = parse_intent_agent\n",
    "#     router = Router(...)\n",
    "#     step2a = coding_agent\n",
    "#     step2b = qa_agent\n",
    "#     step3 = validator\n",
    "# wf = MyWorkflow(); print(wf.run(\"User question\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
